---
title: 'Logistic Regression: Part II - Varietal adoption dataset'
author: Deependra Dhakal
date: '2020-08-07'
slug: logistic-regression-part-ii-varietal-adoption-dataset
categories:
  - R
  - tidyverse
tags:
  - R
  - tidyverse
subtitle: ''
summary: ''
authors: []
lastmod: '2020-08-07T12:27:48+05:45'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: yes
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=4, 
                      echo=TRUE, warning=FALSE, 
                      message=FALSE, cache = TRUE)
require(tidyverse)
require(HSAUR)
require(cowplot)
require(ggridges)
library(gridExtra)
library(tidyselect)
theme_set(theme_bw())
```

## Binary classifier using categorical predictor

Let's say we have two variable -- survey response of farmer to willingness to adopt improved rice variety (in YES/NO) and them having been trained earlier about agricultural input management (in trained/untrained).

Read in the data and notice the summary.

```{r rice-read}
rice_data <- readxl::read_xlsx(here::here("content", "blog", "data", "rice_variety_adoption.xlsx")) %>% 
  mutate_if(.predicate = is.character, as.factor)

rice_variety_adoption <- readxl::read_xlsx(here::here("content", "blog", "data", "rice_variety_adoption.xlsx")) %>%
  select(improved_variety_adoption, training) %>% 
  # convert data to suitable factor type for analysis.
  mutate_if(is.character, as.factor)

head(rice_variety_adoption) # now we have data
```

As a basic descriptive, contruct one way and two way cross tabulation summary, showing count of each categories. This is because logistic regression uses count data, much like in a non-parametric model.

```{r rice-cross-tabs}
# count the data in a one way and two way tabulation
rice_variety_adoption %>% # fortunately there are no rare class (i.e. some class have exceptionally low frequencies)
  count(improved_variety_adoption)

rice_variety_adoption %>% # fortunately there are no rare class (i.e. some class have exceptionally low frequencies)
  count(training)

# # two way cross tabs
# rice_variety_adoption %>% 
#   count(improved_variety_adoption, training) %>% 
#   tidyr::pivot_wider(id_cols = improved_variety_adoption:training, 
#                      names_from = training, values_from = n)

# two way cross tabs (confusion matrix) using yardstick
yardstick::conf_mat(table(rice_variety_adoption)) # class: conf_mat
```

The data representation quickly shows that most of the non adopters have not received training 17, compared to non-adopters that have received training 1. Similarly, the odds that a farmer will be a non adopter if he is trained versus untrained is: 1/17(too low); while the odds that a farmer will be an adopter if he is trained versus untrained is: 16/26 (relatively high).

To statistically verify, we formuate a generalized linear model that uses training to predict adoption of improved variety.

```{r rice-glm}
rice_glm <- glm(improved_variety_adoption ~ training, data = rice_variety_adoption, family = "binomial")
rice_glm %>% summary()
```

The intercept is the log(odds) an untrained farmer will adopt the technolgy. This is because the "No" is the first factor in training variable. The odds is not significant; meaning that odds that an untrained farmer will adopt technology is not different from zero. This can be directly derived as: `26/17` = `r exp(coef(rice_glm))[1]`

The coefficient on training variable is significant and positive. This means that receiving training increases the odds of a farmer being an adopter significantly. Numerically, odds of being an adopter farmer are, on a log scale, 1.619 greater if the famerhas received a training. This can be directly derived as: `log((17/1) / (26/16))` = `r exp(coef(rice_glm))[2]`

Now, we can calculate the overall "Pseudo R-squared" and its p-value. For logistic regression,

```{r rice-pseudo-rsq}
## NOTE: Since we are doing logistic regression...
## Null devaince = 2*(0 - LogLikelihood(null model))
##               = -2*LogLikihood(null model)
## Residual deviacne = 2*(0 - LogLikelihood(proposed model))
##                   = -2*LogLikelihood(proposed model)
ll.null <- rice_glm$null.deviance/-2
ll.proposed <- rice_glm$deviance/-2

## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
```
 
Likewise based on $\chi^2$ values, we can arrive at p-values.

```{r rice-pvalue}

## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((rice_glm$null.deviance - rice_glm$deviance), df=1)
```

 
I is also informative to see in terms of what this logistic regression predicts, given that a farmer is either trained or untrained (and no other data about them).

```{r rice-prediction}
rice_predicted <- tibble(
  probability_of_adoption=rice_glm$fitted.values,
  training=rice_variety_adoption$training)

## We can plot the data...
ggplot(data=rice_predicted, aes(x=training, y=probability_of_adoption)) +
  geom_point(aes(color=training), size=5) +
  xlab("Training") +
  ylab("Predicted probability of a farmer being an adopter") +
  theme_bw()
```
 
Since there are only two probabilities of adopting/non-adopting (one for trained and one for untrained), we can use a table to summarize the predicted probabilities.

```{r rice-class-probabilities}
xtabs(~ probability_of_adoption + training, data=rice_predicted)
```

In a similar way multiple variables can be used as regressors to predict probability of farmer being an adopter of improved variety

## Continuous predictor

```{r example-classification}
rice_variety_adoption <- glm(improved_variety_adoption ~ farm_size, data = rice_data, family = binomial())

# also note that continuous response variables may infact be resulting form composite distributions

rice_data %>% 
  ggplot() +
  geom_boxplot(aes(improved_variety_adoption, farm_size, fill = improved_variety_adoption), alpha = 0.6) +
  geom_violin(aes(improved_variety_adoption, farm_size, fill = improved_variety_adoption), alpha = 0.6) +
  scale_fill_discrete(guide = FALSE)

# break down globulin data into based on arbitrary value of 32
# check if the distribution is normal ?
rice_data %>% 
  ggplot() +
  geom_density_ridges(aes(x = farm_size, y = training, fill = improved_variety_adoption), 
              position = "identity", 
              alpha = 0.6, scale = 1.1, rel_min_height = 0.05) + # `rel_min_height` is minimum height of density below which is ignored in display
  scale_fill_discrete(guide = FALSE)

```

There is likely a presence of two peaks in the category receiving training.

```{r frequency-tab}
rice_freq_tab_char <- rice_data %>% 
  select_if(.predicate = function(x)is.character(x)|is.factor(x))

rice_freq_tab_char <- rice_freq_tab_char %>% 
  pivot_longer(household_head_gender:last_col(), names_to = 'categorical_vars', values_to = "var_value") %>% 
  group_by(categorical_vars, var_value) %>% 
  count(sort = F) %>% 
  ungroup()
```

Pie charts of all categorical variables.

```{r frequency-pie, fig.width=4}
rice_freq_tab_split <- rice_freq_tab_char %>%
  group_split(categorical_vars) %>%
  set_names(map(., ~unique(.x$categorical_vars)))

fpie <- map2(.x = rice_freq_tab_split,
             .y = names(rice_freq_tab_split),
             .f = ~ ggplot(.x,
                           aes(x="", y=n, fill=var_value)) +
               geom_bar(stat="identity", width=1) +
               # Convert to pie (polar coordinates) and add labels
               coord_polar("y", start=0) +
               ggrepel::geom_text_repel(aes(label = scales::percent(round(n/sum(n), 2))),
                         position = position_stack(vjust = 0.5), size = 4) +
               # Add color scale (hex colors)
               # scale_fill_manual(values=c("#55DDE0", "#33658A", "#2F4858",
               #                            "#F6AE2D", "#F26419")) +
               # use fill_viridis_d() for discrete values
               scale_fill_viridis_d(na.value = "grey50") +
               # faceting will add label to individual plot at least
               # facet_wrap(~`Socio-economic variables`) +
               # Remove labels and add title
               labs(x = NULL, y = NULL,
                    fill = NULL,
                    title = str_to_sentence(str_replace_all(.y, "_", " "))) +
               # Tidy up the theme
               theme_classic() +
               theme(axis.line = element_blank(),
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     plot.title = element_text(hjust = 0.5, color = "#666666"))
)

fpie

# # arrange pie plots in grid
# # grid.arrange(grobs = fpie, ncol = 3,
# #              top = "Percentage of response category for socio-economic variables in household samples") ## display plot
# ggsave(filename = "./socio_economic_pie.png",
#        arrangeGrob(grobs = fpie, ncol = 3,
#                    top = "Percentage of response category for categorical variables from surveyed households"),
#        width = 18, height = 18, device = "png", dpi = 300) ## save plot

```

Aggregating grouped variables for regression modeling.

```{r rice-glm-model}
# rice_data %>% skimr::skim()

rice_data_na_deselected <- rice_data %>% 
  select(-shared_landsize, -irrigated_land_size) %>% # shared_landsize and irrigated_land_size both are continuous and have only 0.37, 0.67 complete
  mutate_if(is.character, as.factor)

# collapse factors
rice_data_na_deselected <- rice_data_na_deselected %>%
  mutate(other_income_source = fct_collapse(fct_explicit_na(other_income_source, na_level = "No"), 
                                            Yes = c("Business", "Livestock", "Off-farm job", "Remittance"), 
                                            No = c("No other source"))) %>% 
  mutate(age_group = fct_collapse(age_group, "20-39" = c("20-29", "30-39"), ">=40" = c("40-49", "50-59", "above 60"))) %>%
  mutate(land_size = fct_collapse(land_size, "5-15 kattha" = c("5-15 kattha", "5-15 Kattha"))) %>% 
  mutate(education = fct_collapse(education, 
                                  "Primary or informal" = c("primary education", "Non-formal education only"), 
                                  "Upto 10th" = c("up to SLC"), 
                                  "Intermediate or higher" = c("Intermediate/+2 level", "Bachelors")))

rice_data_na_deselected %>% skimr::skim()
```


## GLM fit

```{r}
rice_glm_fit <- glm(formula = improved_variety_adoption ~ 
                      household_head_gender +
                      age_group +
                      education +
                      land_size +
                      other_income_source +
                      improved_variety_adoption +
                      training +
                      family_size +
                      farm_size, 
                    data = rice_data_na_deselected, family = "binomial")
rice_glm_fit %>% 
  summary()

# write summary table
rice_glm_fit %>% 
  broom::tidy() %>% 
  mutate_if(is.numeric, list(~round(., 3)))
```

## GLMER fit

```{r}
rice_glmer_fit <- lme4::glmer(formula = improved_variety_adoption ~ 
                      (1| household_head_gender) +
                      (1| age_group) +
                      (1| education) +
                      land_size +
                      other_income_source +
                      improved_variety_adoption +
                      training +
                      family_size +
                      farm_size, 
                    data = rice_data_na_deselected, family = "binomial")
rice_glmer_fit %>% 
  summary()

rice_glmer_fit %>% # this is hightly significant
  emmeans::emmeans("training")
```


## Contingency $\chi^2$ for test of independence between proportion of improved variety adopters categories and other categorial variables.

For example,

$H_0$ = The proportion of farm households who adopt improved varieties is independent of the gender of household.

```{r chi-square-contingency}
rice_factors <- names(which((rice_data_na_deselected %>% map_chr(class)) == "factor"))

all_factor_combination <- crossing(Group1 = rice_factors, Group2 = rice_factors) %>% 
  filter(Group1 < Group2)

# # contingeny table chi-square calculation
# # the degrees of freedom equal (number of columns in contingency table minus one) x (number of rows in contingency table minus one) not counting the totals for rows or columns.

# # for all unique combinations
# all_factor_combination %>% 
#   mutate(chi_test = map2(Group1, Group2, 
#                          ~chisq.test(rice_data_na_deselected[, .x, drop = TRUE], 
#                                      rice_data_na_deselected[, .y, drop = TRUE]))) %>% 
#   mutate(chi_stat = map_dbl(chi_test, ~.x$statistic), 
#          chi_pval = map_dbl(chi_test, ~.x$p.value), 
#          chi_method = map_chr(chi_test, ~.x$method), 
#          chi_df = map_dbl(chi_test, ~.x$parameter)) %>% 
#   select(-chi_test) %>% 
#   mutate_if(is.numeric, function(x)round(x, 3))
#   # write_csv("./outputs/rice_chi_square_test_for_independence.csv", "")

# # for improved_variety_adoption only
crossing(Group1 = rice_factors, Group2 = rice_factors) %>% 
  filter(Group1 == "improved_variety_adoption", Group2 != "improved_variety_adoption") %>% 
  mutate(chi_test = map2(Group1, Group2,
                         ~chisq.test(rice_data_na_deselected[, .x, drop = TRUE],
                                     rice_data_na_deselected[, .y, drop = TRUE]))) %>%
  mutate(chi_stat = map_dbl(chi_test, ~.x$statistic),
         chi_pval = map_dbl(chi_test, ~.x$p.value),
         chi_method = map_chr(chi_test, ~.x$method),
         chi_df = map_dbl(chi_test, ~.x$parameter)) %>%
  select(-chi_test) %>%
  mutate_if(is.numeric, function(x)round(x, 3))
```

## Pie chart

```{r pie-chart-dataset}

# # store column names
hd_df_colnames <- rice_data %>%
  colnames() %>%
  enframe() %>%
  .[["value"]]

# # initialize empty list
counted_list <- list()

# # fill up the list with one way tablulation of count
for (var in names(which(map_lgl(rice_data, ~class(.x) %in% c("character", "factor"))))) {
  counted_list[[var]] <- rice_data %>% count(.data[[var]])
}

# # use list to construct pie charts
# # to best filter out any useless plots manually observe long and very wide count tables and omit those
counted_gg_list <- counted_list %>%
  enframe(name = "variable", value = "count_data") %>%
  mutate(variable = str_to_title(str_replace_all(variable, "_", " "))) %>% 
  mutate(plot_count_var = map2(count_data %>%
                                 map(~.x %>% na.omit()),
                               variable,
                               ~ ggplot(data = .x,
                                        aes_string(x = "colnames(.x)[1]",
                                                   y = colnames(.x)[2],
                                                   fill = colnames(.x)[1])) +
                                 geom_bar(stat = "identity", color = "white",
                                          width = 1, size = 1) +
                                 coord_polar(theta = "y", start = 0) +
                                 # scale_fill_discrete(guide = FALSE) +
                                 # guides(fill = guide_legend(title = .y,
                                 #                            title.position = "top")) +
                                 labs(x = "", y = "", title = .y) +
                                 theme_classic() +
                                 ggrepel::geom_text_repel(aes(label = scales::percent(round(n/sum(n),3))),
                                                          position = position_stack(vjust = 0.5))+
                                 theme(axis.line = element_blank(),
                                       axis.text = element_blank(),
                                       axis.ticks = element_blank(),
                                       axis.text.x = element_blank(),
                                       plot.title = element_text(hjust = 0.5, color = "#666666"),
                                       legend.text = element_text(size = 8),
                                       legend.title = element_blank())
  )) %>%
  .[["plot_count_var"]]

# # save the pies in two files
# ggsave(filename = "./outputs/socio_economic_extended_pie1.png",
#        arrangeGrob(grobs = counted_gg_list, ncol = 4,
#                    top = "Percentage of response category for categorical variables from surveyed households"),
#        width = 12, height = 12, units = "in", device = "png", dpi = 280) ## save plot
```


## Binary classifier using logistic regression

In classification problems, the goal is to predict the class membership based on predictors. Often there are two classes and one of the most popular methods for binary classification is logistic regression. The 1958 paper by D. R. Cox is likely to have set the stage for modern regression based approaches in binary output.

The logistic regression, alike several variation in Generalized Linear Model family, is a class of binary regression model whereby the regressand (response variable) is related to predictors via a link function of the corresponding exponential family and by the magnitude of the variance of each measurement is allowed to be a function of its predicted value.

Suppose that $y_1 \in \{0, 1\}$ is a binary class indicator. The conditional response is modeled as $y | x \sim \text{Bernouli}(g_{\beta}(x))$, where

$$g_{\beta}(x) = \frac{1}{1 + \exp^{-x^T \beta}}$$

is the logistic function, and maximize the log-likelihood function, yielding the optimization problem

$$
\begin{array}{ll}
\underset{\beta}{\mbox{maximize}} & \sum_{i = 1}^m \{
y_i \log(g_{\beta}(x_i)) + (1-y_i)\log(1-g_{\beta}(x_i)) \}.
\end{array}
$$

## References

1. YouTube video by CodeEmporium
2. Break the source code of glm.fit()
