<!DOCTYPE html>
<html lang="en" dir="ltr"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.135.0">
<title>Logistic Regression: Part I - Fundamentals | Rookie site</title>


<meta property="twitter:site" content="@dd_rookie">
<meta property="twitter:creator" content="@dd_rookie">







  
    
  
<meta name="description" content="">


<meta property="og:site_name" content="Rookie site">
<meta property="og:title" content="Logistic Regression: Part I - Fundamentals | Rookie site">
<meta property="og:description" content="" />
<meta property="og:type" content="page" />
<meta property="og:url" content="http://localhost:4321/blog/logistic-regression-part-i-fundamentals/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="http://localhost:4321/blog/sidebar-listing.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="http://localhost:4321/blog/sidebar-listing.jpg" >
    
    
  
  <meta itemprop="name" content="Logistic Regression: Part I - Fundamentals">
  <meta itemprop="description" content="Likelihood theory Probit models were the first of those being used to analyze non-normal data using non-linear models. In an early example of probit regression, Bliss(1934) describes an experiment in which nicotine is applied to aphids and the proportion killed is recorded. As an appendix to a paper Bliss wrote a year later (Bliss, 1935), Fisher (1935) outlines the use of maximum likelihood to obtain estimates of the probit model.">
  <meta itemprop="datePublished" content="2020-08-07T00:00:00+00:00">
  <meta itemprop="dateModified" content="2020-08-07T12:19:44+05:45">
  <meta itemprop="wordCount" content="1617">
  <meta itemprop="keywords" content="R,Tidyverse">
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/img/favicon.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.1cbac35dadc4fd962ceb58cdbe3e4bd32eb4a149ef54664a24d670fb5307d543.css" integrity="sha256-HLrDXa3E/ZYs61jNvj5L0y60oUnvVGZKJNZw&#43;1MH1UM=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.36817c360165917b68a008c77f2439837c7c187a0acd3e9c92ce8f23fdc2d825.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="http://localhost:4321/" title="Home">
      <img src="/img/blogophonic-mark-dark.png" class="dib db-l h2 w-auto" alt="Rookie site">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Rookie">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talks/" title="Talks">Talks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/academia/" title="Academia">Academia</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/tipsntricks/" title="Tips and tricks">Tips and tricks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="https://github.com/DeependraD/tex-for-beauty/releases/download/v1.06/Deependra_CV.pdf" title="CV copy">CV</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Logistic Regression: Part I - Fundamentals</h1>
        
        <p class="f6 measure lh-copy mv1">By Deependra Dhakal in <a href="http://localhost:4321/categories/r">R</a>  <a href="http://localhost:4321/categories/tidyverse">tidyverse</a>  <a href="http://localhost:4321/categories/statistics">statistics</a> </p>
        <p class="f7 db mv0 ttu">August 7, 2020</p>

      

      </header>
      <section class="post-body pt5 pb4">
        



<h2 id="likelihood-theory">Likelihood theory
  <a href="#likelihood-theory"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Probit models were the first of those being used to analyze non-normal data using non-linear models. In an early example of probit regression, Bliss(1934) describes an experiment in which nicotine is applied to aphids and the proportion killed is recorded. As an appendix to a paper Bliss wrote a year later (Bliss, 1935), Fisher (1935) outlines the use of maximum likelihood to obtain estimates of the probit model.</p>
<p>However it was years before maximum likelihood estimation for probit models caught on. Finney (1952), in an appendix entitled &ldquo;Mathematical basis of the probit method&rdquo; gives some of the rationale for maximum likelihood and motivates a computational method that he spends six pages describing in a different appendix.</p>
<p>More specifically, if we let <code>\(p_i\)</code> denote the probability of a success for the <code>\(i\)</code>th observation, the probit model is given by</p>
<p>$$
<code>\begin{align} y_i &amp;\sim \text{independent bernoulli} (p_i) \\ p_i &amp;= \Phi(x^\prime_i \beta), \end{align}</code>
(#eq: bernoulli-vector)
$$</p>
<p>Where <code>\(x^\prime_i\)</code> denotes the <code>\(i\)</code>the row of a matrix of predictors and <code>\(\Phi(.)\)</code> is the standard normal CDF. Considering the scale functions applied elementwise to the vectors, we can rewrite (5.1) as</p>
<p>$$
<code>\begin{align} \mathbf{y} &amp;\sim \text{independent bernoulli}(\mathbf{p}) \\ \mathbf{p} &amp;= \mathbf{\Phi(X\beta)} \end{align}</code>
(#eq: bernoulli-matrix)
$$</p>
<p>or equivalently,</p>
<p>$$
\mathbf{\Phi^{-1}(p)} = \mathbf{X \beta},
$$</p>
<p>Where X is the model matrix. The use of the inverse standard normal CDF, known as the probit, to transform the mean of y to the linear predictor is attractive on two counts. First, it expands the range of <code>\(\mathbf{p}\)</code> from [0,1] to the whole real line, making it more reasonable to assume a model of the form <code>\(\mathbf{X \beta}\)</code>. Second, in many problems, the sigmoidal form of <code>\(\mathbf{p}\)</code> as a function of the covariates is often observed in practice.</p>
<p>Finney (1952) suggested calculating an estimate of <code>\(\beta\)</code> via an iteratively weighted least squares algorithm. He recommened using working probits which he defined (ignoring the shift of five units historically used to keep all the calculations positive) as</p>
<p>$$
t_i = x^{\prime}_i \beta + \frac{y_i - \mathbf{\Phi}( x^{\prime}_i \beta)}{\phi(x^{\prime}_i \beta)}
(#eq:working-probits)
$$</p>
<p>Where <code>\(\phi(.)\)</code> is the standard normal probability density function (pdf). The working probits for a current value of <code>\(\beta\)</code> were regressed on the predictors using weights given by <code>\({\large \frac{[\phi(p_i)]^2}{\mathbf{\Phi} (pi)[1-\mathbf{\Phi}(p_i)]}}\)</code> in order to get the new values of <code>\(\beta\)</code>. This algorithm was iterated untill convergence (or at least until the &ldquo;person&rdquo; got tired of performing the calculations).</p>
<p>Nelder and Wedderburn (1972) recognized that the working probits could be generalized in a straightforward way to unify an entire collection of maximum likelihood problems. This generalized linear model (GLM) could handle probit or logistic regression, Poisson regression, log-linear models for contingency tables, variance component estimation from ANOVA mean squares and many other problems in the same way.</p>
<p>They replaced the <code>\(\mathbf{\Phi}^{-1}\)</code> with a general link function, <code>\(g(.)\)</code>, which transforms (or links) the mean of <code>\(y_i\)</code> to the linear predictor. With <code>\(g_\mu(\mu)\)</code> representing <code>\({\large \frac{\delta g(\mu)}{\delta \mu}}\)</code>, they then defined a workign variate via</p>
<p>[
<code>\begin{aligned} t_i &amp;= g(\mu_i) + g_{\mu}(\mu_i)(y_i - \mu_i) \\ &amp;= x^{\prime}_i \beta + g_{\mu}(\mu_i)(y_i - \mu_i) \end{aligned}</code>
(#eq:working-variate)
]</p>
<p>Since the second term of the right-hand side of eq:working-variate has expectation zero it can be regarded as an error term so that <code>\(t_i\)</code> follows a linear model, albeit with unequal variances which depend on the unknown <code>\(\beta\)</code>. This suggests using eq:working-variate just like eq:working-probits: regress <code>\(t\)</code> on <code>\(\mathbf{X}\)</code> using a weighted linear regression and iterate until the estimates of <code>\(\beta\)</code> stabilize.</p>
<p>More important, it made possible a style of thinking which freed the data analyst from necessarily looking for a transformation which simultaneously achieved linearity in the predictors and normality of the distribution (as in Box and Cox, 1962).</p>
<p>What advantages does this have ? First, it unifies what appear to be very different methodologies, which helps us to understand, use and (for those of us in the business) teach the techniques. Second, since the right-hand side of the model equation is a linear model after applying the link, many of the standard ways of thinking about linear models carry over to GLMs.</p>




<h2 id="exponential-function-and-its-distribution">Exponential function and its distribution
  <a href="#exponential-function-and-its-distribution"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/unnamed-chunk-1-1.png" width="672" />
<p>The goal of maximum likelihood is therefore, given a set of measurements, finding optimum value of <code>\(\lambda\)</code>.</p>
<p>So assume that a person collects a lot of data about how much time passed between views of this video.</p>
<ul>
<li><code>\(x_1\)</code> = The amount of time that passed between the 1st and 2nd views.</li>
<li><code>\(x_2\)</code> = The amount of time that passed between the 2nd and 3rd views.</li>
<li><code>\(x_3\)</code> = The amount of time that passed between the 3rd and 4th views.</li>
</ul>
<p>Thus, <code>\(x_1, x_2, x_3, \ldots, x_n\)</code> are n measurement of view intervals.</p>
<p>For now, let&rsquo;s assume that we already have a good value of <code>\(\lambda\)</code>.</p>
<p>Now, what is the likelihood of <code>\(\lambda\)</code> given our first measurement, <code>\(x_1\)</code> ?</p>
<p>The likelihood is given by <code>\(L(\lambda | x_1)=\lambda \exp^{- \lambda x_1}\)</code>.</p>
<p>Similarly, the likelihood of <code>\(\lambda\)</code> given our second measurement, <code>\(x_2\)</code> is given by <code>\(L(\lambda | x_2) = \lambda \exp^{- \lambda x_2}\)</code>, and so on. It is the value of y (in y-axis) obtained by plugging in the value of <code>\(x_1\)</code> and <code>\(x_2\)</code> in the function.</p>
<p>So now, what is the likelihood of <code>\(\lambda\)</code> given both <code>\(x_1\)</code> and <code>\(x_2\)</code> ? We know individual likelihood functions as we present above. The combined likelihood function can be represented multiplication of two likelihood functions,</p>
<p>$$
<code>\begin{align} L(\lambda | x_1 \text{ and } x_2) &amp;= L(\lambda | x_1) L(\lambda | x_2) \\ &amp;= \lambda \exp^{-\lambda x_1}.\lambda \exp^{-\lambda x_2} \\ &amp;= \lambda^2[\exp^{-\lambda x_1}. \exp^{-\lambda x_2}] \\ &amp;= \lambda^2[\exp^{-\lambda(x_1 + x_2)}] \end{align}</code>
$$</p>
<p>The the last equation is the likelihood of <code>\(\lambda\)</code> given <code>\(x_1\)</code> and <code>\(x_2\)</code>.</p>
<p>What is the likelihood of <code>\(\lambda\)</code> given all of the data, <code>\(x_1, x_2, x_3, \ldots, x_n\)</code> ?</p>
<p>It is combined likelihood of observing all data values.</p>
<p>$$
<code>\begin{align} L(\lambda | x_1, x_2, x_3, \ldots, x_n) &amp;= L(\lambda | x_1) L(\lambda | x_2) L(\lambda | x_3) \ldots L(\lambda | x_n) \\ &amp;= \lambda e^{-\lambda x_1}.\lambda e^{-\lambda x_2}.\lambda e^{-\lambda x_3} \\ &amp;= \lambda^n[e^{-\lambda x_1}e^{-\lambda x_2}\ldots e^{-\lambda x_3}] \\ &amp;= \lambda^n[e^{-\lambda(x_1 + x_2 + x_3 + \ldots + x_n)}] \end{align}</code>
(#eq: likelihood-fun)
$$</p>
<p>Now an issue could be, what if we don&rsquo;t have a good value of <code>\(\lambda\)</code> ? One solution is we could try different values for <code>\(\lambda\)</code> to find a good one. To find the maximum likelihood,</p>
<ol>
<li>Take the derivative of likelihood-fun</li>
<li>Solve for <code>\(\lambda\)</code> when the derivative is set to be equal to 0.</li>
</ol>
<p>Now let us again consider the values of <code>\(x_i\)</code> (the time interval between two successive viewership of channes, in our previous example).</p>
<table>
  <thead>
      <tr>
          <th style="text-align: right">x</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">1.5</td>
      </tr>
      <tr>
          <td style="text-align: right">1.6</td>
      </tr>
      <tr>
          <td style="text-align: right">1.7</td>
      </tr>
      <tr>
          <td style="text-align: right">1.8</td>
      </tr>
      <tr>
          <td style="text-align: right">1.9</td>
      </tr>
      <tr>
          <td style="text-align: right">2.0</td>
      </tr>
      <tr>
          <td style="text-align: right">2.1</td>
      </tr>
      <tr>
          <td style="text-align: right">2.2</td>
      </tr>
      <tr>
          <td style="text-align: right">2.3</td>
      </tr>
      <tr>
          <td style="text-align: right">2.4</td>
      </tr>
      <tr>
          <td style="text-align: right">2.5</td>
      </tr>
      <tr>
          <td style="text-align: right">2.6</td>
      </tr>
      <tr>
          <td style="text-align: right">2.7</td>
      </tr>
      <tr>
          <td style="text-align: right">2.8</td>
      </tr>
      <tr>
          <td style="text-align: right">2.9</td>
      </tr>
      <tr>
          <td style="text-align: right">3.0</td>
      </tr>
      <tr>
          <td style="text-align: right">3.1</td>
      </tr>
      <tr>
          <td style="text-align: right">3.2</td>
      </tr>
      <tr>
          <td style="text-align: right">3.3</td>
      </tr>
      <tr>
          <td style="text-align: right">3.4</td>
      </tr>
      <tr>
          <td style="text-align: right">3.5</td>
      </tr>
      <tr>
          <td style="text-align: right">3.6</td>
      </tr>
      <tr>
          <td style="text-align: right">3.7</td>
      </tr>
      <tr>
          <td style="text-align: right">3.8</td>
      </tr>
      <tr>
          <td style="text-align: right">3.9</td>
      </tr>
      <tr>
          <td style="text-align: right">4.0</td>
      </tr>
      <tr>
          <td style="text-align: right">4.1</td>
      </tr>
      <tr>
          <td style="text-align: right">4.2</td>
      </tr>
      <tr>
          <td style="text-align: right">4.3</td>
      </tr>
      <tr>
          <td style="text-align: right">4.4</td>
      </tr>
      <tr>
          <td style="text-align: right">4.5</td>
      </tr>
      <tr>
          <td style="text-align: right">4.6</td>
      </tr>
      <tr>
          <td style="text-align: right">4.7</td>
      </tr>
      <tr>
          <td style="text-align: right">4.8</td>
      </tr>
      <tr>
          <td style="text-align: right">4.9</td>
      </tr>
      <tr>
          <td style="text-align: right">5.0</td>
      </tr>
  </tbody>
</table>
<p>Now the relationship between likelihood values using different sets of <code>\(\lambda\)</code> can be seen as:</p>
<img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/unnamed-chunk-3-1.png" width="672" />
<p>Noticeably, this function is ugly because of too small or too large estimated values. This function initially rises, but finally starts to slow down. This can be seen at the point when likelihood value starts to decrease &ndash; point of inflection. This point can also be obtained by setting the slope of likelihood function (derivative) to 0.</p>
<p>But before that we may take the log of the combined likelihood function. This is because the derivative of a function and the derivative of the log of a function equal 0 at the same place. So, for the purpose of finding where the derivative equals 0, the original function and its log are interchangeable. Thus we have the log of likelihood function is:</p>
<p>log likelihood function can be plotted against lambda looks,</p>
<img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/unnamed-chunk-5-1.png" width="672" />
<p>When we can specify the log form, multiplicative terms of initial function &ndash; <code>\(\lambda\)</code> and <code>\(\exp\)</code> can now be added as shown in derivative of log likelihood function with respect to <code>\(\lambda\)</code>,</p>
<p>$$
<code>\begin{align} \frac{\delta}{\delta \lambda} L(\lambda | x_1, x_2, x_3, \ldots, x_n) &amp;= \frac{\delta}{\delta \lambda} \log (\lambda^n[\exp^{-\lambda(x_1 + x_2 + x_3 + \ldots + x_n)}]) \\ &amp;= \frac{\delta}{\delta \lambda} \log (\lambda^n) + \log[\exp^{-\lambda(x_1 + x_2 + x_3 + \ldots + x_n)}]) \\ &amp;= \frac{\delta}{\delta \lambda} n\log (\lambda) -\lambda(x_1 + x_2 + x_3 + \ldots + x_n) \end{align}</code>
$$</p>
<p>Finally taking the derivative, and setting the value to 0, we get:</p>
<p>$$
0 = n \frac{1}{\lambda}-(x_1 + x_2 + x_3 + \ldots + x_n)
$$</p>
<p>Solving for <code>\(\lambda\)</code> give,</p>
<p>$$
\lambda = \frac{n}{(x_1 + x_2 + x_3 + \ldots + x_n)}
$$</p>
<p>Now, whenever we collect a lot of data about how much time takes place between events, we just plug those values into this equation and we&rsquo;ll get the maximum likelihood estimate for <code>\(\lambda\)</code> and then we can fit an exponential distribution to our data.</p>
<p>Hence, we can see that, for example, if 2 seconds passed between 1st and 2nd times a video was watched <code>\((x_1 = 2)\)</code>, 2.5 seconds passed between 2nd and 3rd times the video was watched <code>\((x_2 = 2.5)\)</code> and 1.5 seconds passed between 3rd and 4th time the video was watched <code>\((x_3 = 1.5)\)</code>, we can have the maximum likelihood estimate,</p>
<p>$$
\lambda = \frac{3}{2 + 2.5 + 1.5} = 0.5
$$</p>
<p>Thus, given the data, the maximum likelihood estimate for <code>\(\lambda\)</code> is 0.5. Graphically,</p>
<img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/unnamed-chunk-6-1.png" width="672" />
<pre tabindex="0"><code>## `geom_line()`: Each group consists of only one observation.
## ℹ Do you need to adjust the group aesthetic?
</code></pre><img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/unnamed-chunk-7-1.png" width="672" />




<h2 id="inverse-function-and-its-distribution">Inverse function and its distribution
  <a href="#inverse-function-and-its-distribution"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<img src="/blog/logistic-regression-part-i-fundamentals/index_files/figure-html/inverse-function-1.png" width="672" />




<h2 id="references">References
  <a href="#references"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>




<h3 id="interpretation">Interpretation
  <a href="#interpretation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<ol>
<li>
<a href="https://www.youtube.com/watch?v=JC56jS2gVUE" target="_blank" rel="noopener">Deviance Residuals</a></li>
<li>
<a href="https://www.youtube.com/watch?v=8nm0G-1uJzA" target="_blank" rel="noopener">StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!!</a></li>
<li>
<a href="https://www.youtube.com/watch?v=vN5cNN2-HWE" target="_blank" rel="noopener">Logistic Regression Details Pt1: Coefficients</a></li>
<li>
<a href="https://www.youtube.com/watch?v=BfKanl1aSG0" target="_blank" rel="noopener">Logistic Regression Details Pt 2: Maximum Likelihood</a></li>
<li>
<a href="https://www.youtube.com/watch?v=xxFYro8QuXA" target="_blank" rel="noopener">Logistic Regression Details Pt 3: R-squared and p-value</a></li>
<li>
<a href="https://www.youtube.com/watch?v=C4N3_XJJ-jU" target="_blank" rel="noopener">Logistic Regression in R, Clearly Explained!!!!</a></li>
</ol>




<h3 id="examples">Examples
  <a href="#examples"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<ol>
<li>A Handbook of Statistical Analyses Using R 
<a href="https://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_logistic_regression_glm.pdf" target="_blank" rel="noopener">pdf</a></li>
<li>Heart disease dataset and analysis 
<a href="https://raw.githubusercontent.com/StatQuest/logistic_regression_demo/master/logistic_regression_demo.R" target="_blank" rel="noopener">script</a> 
<a href="http://archive.ics.uci.edu/ml/datasets/Heart&#43;Disease" target="_blank" rel="noopener">Dataset</a></li>
</ol>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">August 7, 2020</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">8 minute read, 1617 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="http://localhost:4321/categories/r">R</a>  <a href="http://localhost:4321/categories/tidyverse">tidyverse</a>  <a href="http://localhost:4321/categories/statistics">statistics</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="http://localhost:4321/tags/r">R</a>  <a href="http://localhost:4321/tags/tidyverse">tidyverse</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/blog/cluster-dendrogram-an-introduction-and-showcase/">Cluster dendrogram: An introduction and showcase</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/paste-together-multiple-columns/">Paste together multiple columns</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/logistic-regression-part-ii-varietal-adoption-dataset/">Logistic Regression: Part II - Varietal adoption dataset</a></dd>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="http://localhost:4321/blog/linear-model-fitting-for-regression-basics-and-variation/">&larr; Linear model fitting for regression: Basics and Variation</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="http://localhost:4321/blog/logistic-regression-part-ii-varietal-adoption-dataset/">Logistic Regression: Part II - Varietal adoption dataset &rarr;</a>
  
</div>

      </footer>
    </article>
    
      <div class="post-comments pa0 pa4-l mt4">
  
    
      <script src="https://utteranc.es/client.js"
              repo="deependrad/dblog"
              issue-term="pathname"
              theme="boxy-light"
              label="comments :crystal_ball:"
              crossorigin="anonymous"
              async
              type="text/javascript">
      </script>
    
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2024 Dblog
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/deependrad/dblog" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://bsky.app/" title="bluesky" target="_blank" rel="me noopener">
      <i class="fab fa-bluesky fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/dd_rookie" title="x-twitter" target="_blank" rel="me noopener">
      <i class="fab fa-x-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/0000-0002-0347-7950" title="orcid" target="_blank" rel="me noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/blog/index.xml" title="rss" >
      <i class="fas fa-rss fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/contributors/" title="Contributors">Contributors</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
